{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Groups Data Scraper\n",
    "\n",
    "## Overview\n",
    "This Python script is designed to scrape thread titles, URLs, and post details from all pages of Google Groups search results based on a specified group and search term. The results are comprehensively saved into a CSV file. This script is tailored for the group \"alt.conspiracy.area51\" with the search term \"lazar\".\n",
    "\n",
    "## Functionality\n",
    "- **Pagination Handling**: Navigates through all available pages of search results.\n",
    "- **Data Extraction**: Retrieves thread titles, URLs, authors, timestamps, and content of each post.\n",
    "- **Error Handling**: Implements robust error handling to manage network issues and missing data.\n",
    "- **Data Storage**: Results are saved into a CSV file, named with the group, search term, and the timestamp of when the file was generated.\n",
    "\n",
    "## Output File\n",
    "The output CSV file is named in the format `{group_name}_{search_term}_{timestamp}.csv`, making it easy to identify and organize files based on the group, search term, and the time of data scraping.\n",
    "\n",
    "## Libraries Used\n",
    "- `requests`: For performing HTTP requests.\n",
    "- `beautifulsoup4`: For parsing HTML and XML documents.\n",
    "- `csv`: For writing the data into a CSV format.\n",
    "- `datetime`: For generating the current timestamp to append to the output file name.\n",
    "\n",
    "## Usage\n",
    "To use this script, ensure you have the necessary Python libraries installed and execute the script in an environment where you have permissions to read from websites and write files. This script assumes compliance with Google's Terms of Service and the appropriate use of data scraped from the internet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to sci.space_test_20240707_183529.csv\n"
     ]
    }
   ],
   "source": [
    "# FINAL VERSION\n",
    "# This script scrapes thread titles, URLs, and post details from all pages of Google Groups search results for a given query and group.\n",
    "# It saves the scraped data into a CSV file with the group name, search term, and current time of file generation included in the filename.\n",
    "# The script includes error handling to avoid NoneType errors and implements pagination to gather data from all result pages.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from datetime import datetime\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "\n",
    "# Hardcoded inputs for group and search term\n",
    "# group_name = \"alt.conspiracy.area51\"\n",
    "# search_term = \"disinfo\"\n",
    "\n",
    "group_name = \"sci.space\"\n",
    "search_term = \"test\"\n",
    "\n",
    "# URL of the Google Groups search page\n",
    "base_url = \"https://groups.google.com\"\n",
    "search_url = f\"https://groups.google.com/g/{group_name}/search?q={search_term}&pli=1\"\n",
    "\n",
    "# Create a session with retry functionality\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "# Data storage for collected posts\n",
    "all_posts = []\n",
    "\n",
    "\n",
    "def get_threads_on_page(url):\n",
    "    try:\n",
    "        response = session.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for 4xx or 5xx status codes\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "        threads = soup.find_all(\"div\", class_=\"cXEmmc\")\n",
    "\n",
    "        for thread in threads:\n",
    "            title_element = thread.find(\"div\", class_=\"t17a0d\")\n",
    "            title = title_element.text.strip() if title_element else \"No Title\"\n",
    "            link_element = thread.find(\"a\", class_=\"ZLl54\")\n",
    "            thread_url = base_url + link_element[\"href\"] if link_element else \"No URL\"\n",
    "            thread_response = session.get(thread_url)\n",
    "            thread_response.raise_for_status()\n",
    "            thread_soup = BeautifulSoup(thread_response.content, \"html.parser\")\n",
    "            posts = thread_soup.find_all(\"section\", class_=\"BkrUxb\")\n",
    "\n",
    "            for post in posts:\n",
    "                content_element = post.find(\"div\", class_=\"ptW7te\")\n",
    "                content = (\n",
    "                    \" \".join(p.text.strip() for p in content_element.find_all(\"p\"))\n",
    "                    if content_element\n",
    "                    else \"No Content\"\n",
    "                )\n",
    "                author_element = post.find(\"h3\", class_=\"s1f8Zd\")\n",
    "                author = author_element.text.strip() if author_element else \"No Author\"\n",
    "                timestamp_element = post.find(\"span\", class_=\"zX2W9c\")\n",
    "                timestamp = (\n",
    "                    timestamp_element.text.strip()\n",
    "                    if timestamp_element\n",
    "                    else \"No Timestamp\"\n",
    "                )\n",
    "                all_posts.append(\n",
    "                    {\n",
    "                        \"Thread Title\": title,\n",
    "                        \"Thread URL\": thread_url,\n",
    "                        \"Author\": author,\n",
    "                        \"Timestamp\": timestamp,\n",
    "                        \"Content\": content,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        return soup\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"An error occurred while making the request:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_all_pages():\n",
    "    next_page_url = search_url\n",
    "    while next_page_url:\n",
    "        soup = get_threads_on_page(next_page_url)\n",
    "        if soup is None:\n",
    "            break\n",
    "        next_page_element = soup.find(\"a\", class_=\"G0iuSb\")\n",
    "        next_page_url = (\n",
    "            base_url + next_page_element[\"href\"] if next_page_element else None\n",
    "        )\n",
    "\n",
    "\n",
    "# Start the process to get all pages\n",
    "get_all_pages()\n",
    "\n",
    "# Save data to a CSV file\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"{group_name}_{search_term}_{timestamp}.csv\"\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.DictWriter(\n",
    "        file,\n",
    "        fieldnames=[\"Thread Title\", \"Thread URL\", \"Author\", \"Timestamp\", \"Content\"],\n",
    "    )\n",
    "    writer.writeheader()\n",
    "    for post in all_posts:\n",
    "        writer.writerow(post)\n",
    "\n",
    "print(f\"Data has been saved to {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Newcodeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
